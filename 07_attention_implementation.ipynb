{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from math import sqrt\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "gA8Y_Qb6c1H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "\n",
        "Attention is All You Need\n",
        "\n",
        "Google Brain\n",
        "\n",
        "2017"
      ],
      "metadata": {
        "id": "oxKz0-Zfiwfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model](https://drive.google.com/uc?id=1VcJ2iqMMvNA1HtsXQukio9V0i6nEvWMK)"
      ],
      "metadata": {
        "id": "MS6TfsEdhf5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Sub-Layers Implementation\n",
        "\n",
        "### 1.1 Positional Encoding"
      ],
      "metadata": {
        "id": "MFxFT7W9jO65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module): \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size,\n",
        "                                             config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
        "                                                config.hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Create position IDs for input sequence\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
        "        # Create token and position embeddings\n",
        "        token_embeddings = self.token_embeddings(input_ids) \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        # Combine token and position embeddings\n",
        "        embeddings = token_embeddings + position_embeddings\n",
        "        embeddings = self.layer_norm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "zrdQhkbTJniH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Multi-Head Attention\n",
        "\n",
        "![multi_head_attention](https://drive.google.com/uc?id=17xC_ptWBzoRjZch1z1q7orHZxaKZYH-t)"
      ],
      "metadata": {
        "id": "wim76mmGjx4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, embed_dim, head_dim):\n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(embed_dim, head_dim)\n",
        "        self.k = nn.Linear(embed_dim, head_dim)\n",
        "        self.v = nn.Linear(embed_dim, head_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        attn_outputs = scaled_dot_product_attention(\n",
        "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state), mask)\n",
        "        return attn_outputs"
      ],
      "metadata": {
        "id": "FJjBL56o9YjD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config, embed_dim, head_dim):\n",
        "        super().__init__()\n",
        "        embed_dim = config.hidden_size\n",
        "        num_heads = config.num_attention_heads\n",
        "        head_dim = embed_dim // num_heads\n",
        "        self.heads = nn.ModuleList(\n",
        "            [MultiHeadAttention(embed_dim, head_dim) for _ in range(num_heads)]\n",
        "        )\n",
        "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
        "        x = self.output_linear(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "vE6_4DXl5oKQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Scaled Dot-Product Attention\n",
        "\n",
        "![scaled_dot_proudct_attention](https://drive.google.com/uc?id=1VLoOdj16tctQ4YLF8vfHl5Quoc-mvT3q)"
      ],
      "metadata": {
        "id": "YPExfG6xkOYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    dim_k = query.size(-1)\n",
        "    scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "    weights = F.softmax(scores, dim=-1)\n",
        "    return weights.bmm(value)"
      ],
      "metadata": {
        "id": "1PuPgH6Nx4KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Position-Wise Fully Connected Feed-Forward Network"
      ],
      "metadata": {
        "id": "hDTADEvSlJYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "619LLxIBCXsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Encoder Implementation\n",
        "\n",
        "![enc_dec](https://drive.google.com/uc?id=1VUvTRuCWMCsVYbVftkIcmz63VuCc0VgL)"
      ],
      "metadata": {
        "id": "Q7xkH3S5nK4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Encoder Layer"
      ],
      "metadata": {
        "id": "cqy-EFkOlXBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # [1, 5, 768]\n",
        "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
        "        # [1, 32, 768]\n",
        "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply layer norma & copy input into a q, k, v\n",
        "        hidden_state = self.layer_norm_1(x)\n",
        "        # apply attention with a skip connection\n",
        "        x = x + self.attention(hidden_state)\n",
        "        # apply feed-forward with a skip connection\n",
        "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "6p7YngCnuMHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Encoder"
      ],
      "metadata": {
        "id": "pCIUNyzuniC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module): \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embeddings = PositionalEncoding(config)\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(config)\n",
        "                                    for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x) \n",
        "        for layer in self.layers:\n",
        "            x = layer(x) \n",
        "        return x"
      ],
      "metadata": {
        "id": "WifBJTPiMTLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Decoder Implementation\n",
        "\n",
        "![enc_dec](https://drive.google.com/uc?id=1_lfa-u_O6JaAyYRR7Dtue6db3KV-kDFI)"
      ],
      "metadata": {
        "id": "OsVaXOBonbXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Decoder Layer"
      ],
      "metadata": {
        "id": "KF6_BhxvoRl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "\n",
        "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.enc_dec_attention = MultiHeadAttention(config)\n",
        "\n",
        "        self.layer_norm_3 = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "\n",
        "    def forward(self, x_dec, x_enc, target_mask, src_mask):\n",
        "        # apply layer norm & copy input into a q, k, v\n",
        "        hidden_state_attention = self.layer_norm_1(x_dec)\n",
        "        # apply masked attention with a skip connection \n",
        "        x = x_dec + self.attention(hidden_state_attention, target_mask)\n",
        "\n",
        "        if x_enc is not None:\n",
        "            # apply layer norm & copy input into a q, k, v\n",
        "            hidden_state_enc_dec = self.layer_norm_2(x)\n",
        "            # apply masked attention with a skip connection\n",
        "            x = x + self.enc_dec_attention(hidden_state_enc_dec, src_mask)\n",
        "\n",
        "        # apply feed-forward with a skip connection\n",
        "        x = self.feed_forward(self.layer_norm_3(x))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "KGKYaREoyzj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Decoder"
      ],
      "metadata": {
        "id": "9zAquC_roVen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embeddings = PositionalEncoding(config)\n",
        "        self.layers = nn.ModuleList([TransformerDecoderLayer(config)\n",
        "                                    for _ in range(config.num_hidden_layers)])\n",
        "        \n",
        "        self.linear = nn.Linear()\n",
        "        \n",
        "    def forward(self, target, source, target_mask, src_mask):\n",
        "        target = self.embeddings(target)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(target, source, target_mask, src_mask)\n",
        "\n",
        "        out = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        return out"
      ],
      "metadata": {
        "id": "gUKL-hLCYpTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. References\n",
        "\n",
        "- [Attention is All You Need, 2017 - Google](https://arxiv.org/abs/1706.03762)\n",
        "- [The Illustrated Transformer - Jay Alammar](http://jalammar.github.io/illustrated-transformer/)\n",
        "- [Data & Optimization Code Reference - Bentrevett](https://github.com/bentrevett/pytorch-seq2seq/)"
      ],
      "metadata": {
        "id": "MdBpsi81qk6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contenido\n",
        "cubierto hasta ahora\n",
        "\n",
        "1. Historia AI\n",
        "2. Intro AI\n",
        "3. Intro Transformers\n",
        "4. Text clasification\n",
        "5. Transformer Arquitecture\n",
        "\n",
        "## Transformers\n",
        "Opciones para cubrir (escoger 4)\n",
        "\n",
        "* Multilingual NER\n",
        "* Text Generation\n",
        "* Summarization\n",
        "* Q&A\n",
        "* Transformers in prod\n",
        "* Training transformers from scratch\n",
        "\n",
        "## Computer Vision\n",
        "\n",
        "## Reinforcemente Learning"
      ],
      "metadata": {
        "id": "VSvsqYIdgMxS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KFwLDlaOuctY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}