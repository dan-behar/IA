{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU was detected! This notebook can be *very* slow without a GPU üê¢\n",
            "Using transformers v4.11.3\n",
            "Using datasets v1.16.1\n"
          ]
        }
      ],
      "source": [
        "from utils import *\n",
        "setup_lecture()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZR5DIsOkMNL"
      },
      "source": [
        "## Entrenando un clasificador de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpwQd29ckMNL"
      },
      "source": [
        "Como vimos al principio del notebook anterios, los modelos como DistilBERT est√°n preentrenados para predecir palabras enmascaradas en una secuencia de texto. Sin embargo, no podemos usar estos modelos de lenguaje directamente para la clasificaci√≥n de texto; tenemos que modificarlos un poco. Para saber qu√© modificaciones son necesarias, veamos la arquitectura de un modelo basado en una arquitectura encoder-decoder como DistilBERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4bZVRUAkMNL"
      },
      "source": [
        "Primero, el texto es tokenizado y representado como vectores one-hot llamados _token encodings_. El tama√±o del vocabulario del tokenizador determina la dimensi√≥n de las codificaciones de tokens y, por lo general, son de 20k a 200k tokens √∫nicos. Luego, estas codificaciones de tokens se convierten en _token embeddings_, que son vectores que viven en un espacio de menor dimensi√≥n. Los _toke embeddings_ luego se pasan a trav√©s de las capas del bloque del encoder para generar un _hidden state_ para cada token de input. Para el objetivo de preentrenamiento del modelado del lenguaje, cada _hidden state_ se env√≠a a una capa que predice los tokens de input enmascarados. Para la tarea de clasificaci√≥n, reemplazamos la capa de modelado de lenguaje con una capa de clasificaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khSA1qsvkMNL"
      },
      "source": [
        "> nota: En la pr√°ctica, PyTorch omite el paso de crear vectores one-hot para codificaciones de tokens porque multiplicar una matriz con un vector one-hot es lo mismo que seleccionar una columna de la matriz. Esto se puede hacer directamente obteniendo la columna con el ID del token de la matriz. Veremos esto a detalle cuando usemos la clase `nn.Embedding`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OreHf-TykMNL"
      },
      "source": [
        "Tenemos dos opciones para entrenar un modelo en nuestro conjunto de datos de Twitter:\n",
        "\n",
        "- _Feature extraction_: Usamos los hidden states como features y solo entrenamos un clasificador en ellos, sin modificar el modelo preentrenado.\n",
        "\n",
        "- _Fine-tuning_: Entrenamos todo el modelo de extremo a extremo, lo que tambi√©n actualiza los par√°metros del modelo preentrenado.\n",
        "\n",
        "En las siguientes secciones exploramos ambas opciones para DistilBERT y vemos sus trade-offs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-ZRhXPgkMNL"
      },
      "source": [
        "### Transformers como extractores de features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk3K0m6PkMNL"
      },
      "source": [
        "Usar un transformer como feature extractor es bastante simple. Primero, congelamos los pesos del cuerpo durante el entrenamiento y usamos los hidden states como features para el clasificador. La ventaja de este enfoque es que podemos entrenar r√°pidamente un modelo peque√±o o poco profundo. Dicho modelo podr√≠a ser una capa de clasificaci√≥n o un m√©todo que no dependa de gradientes, como un random forrest. Este m√©todo es especialmente conveniente si no tenemos GPUs, ya que los hidden states solo deben calcularse una vez."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73obJ5tkkMNL"
      },
      "source": [
        "#### Usando modelos preentrenados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uywUBURskMNL"
      },
      "source": [
        "Vamos a usar otra _auto class_ conveniente de Transformers llamada `AutoModel`. Similar a la clase `AutoTokenizer`, `AutoModel` tiene un m√©todo `from_pretrained()` para cargar los pesos de un modelo que ya ha sido entrenado. Usemos este m√©todo para cargar el checkpoint de DistilBERT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ys5rLkHRkMNM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model_ckpt = \"distilbert-base-uncased\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModel.from_pretrained(model_ckpt).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRLZ1ArWkMNM"
      },
      "source": [
        "Aqu√≠ usamos PyTorch para verificar si hay un GPU disponible o no, y luego encadenamos el m√©todo `nn.Module.to()` de PyTorch al cargador de modelos. Esto asegura que el modelo se ejecutar√° en la GPU si tenemos una. De lo contrario, el modelo se ejecutar√° en la CPU, que puede ser m√°s lenta.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwMKTplPkMNM"
      },
      "source": [
        "La clase `AutoModel` convierte los _token encodings_ en _token embeddings_ y luego las pasa al encoder stack para devolver los _hidden states_. Veamos c√≥mo extraer estos estados de nuestro corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E_hXijrkMNM"
      },
      "source": [
        "### Apartado: Interoperabilidad entre frameworks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QScVxlbakMNM"
      },
      "source": [
        "Aunque el c√≥digo que vamos a usar est√° principalmente en PyTorch, Transformers tiene buena interoperabilidad con TensorFlow y JAX. Esto significa que solo necesitan cambiar un par l√≠neas de c√≥digo para cargar un modelo preentrenado en su framework preferido! Por ejemplo, podemos cargar DistilBERT en TensorFlow usando la clase `TFAutoModel`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g-s-MLDukMNM",
        "outputId": "bf175797-9be8-42d4-900a-062476484411"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "tf_model = TFAutoModel.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM-flL10kMNM"
      },
      "source": [
        "Esta interoperabilidad es especialmente √∫til cuando un modelo solo se publica en un framework, pero nos gustar√≠a usarlo en otro. Por ejemplo, el [modelo XLM-RoBERTa](https://huggingface.co/xlm-roberta-base) que vamos a usar para NER mas adelante, solo tiene pesos de PyTorch, por lo que si intentamos cargarlo en TensorFlow como hicimos antes:\n",
        "\n",
        "```python\n",
        "tf_xlmr = TFAutoModel.from_pretrained(\"xlm-roberta-base\")\n",
        "```\n",
        "\n",
        "Nos va a dar un error. En estos casos, podemos especificar el argumento `from_pt=True` en el metodo `TfAutoModel.from_pretrained()`, y la libreria automaticamente va a bajar y convertir los pesos de PyTorch por uno:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "B6BQy4QGkMNM"
      },
      "outputs": [],
      "source": [
        "tf_xlmr = TFAutoModel.from_pretrained(\"xlm-roberta-base\", from_pt=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd2Tv03xkMNM"
      },
      "source": [
        "Como puedem ver, es facil cambiar entre frameworks en Transformers. En la mayor√≠a de los casos, solo pueden agregar un prefijo \"TF\" a las clases y van a tener las clases equivalentes de TensorFlow 2.0. Cuando usamos el string `\"pt\"` (por ejemplo, en la siguiente secci√≥n), que es la abreviatura de PyTorch, simplemente reempl√°cenla con \"`tf\"`, que es la abreviatura de TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV6EE20kkMNM"
      },
      "source": [
        "### Fin de apartado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95sMz_6YkMNM"
      },
      "source": [
        "#### Extrayendo los ultimos _hidden states_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrPkk85MkMNN"
      },
      "source": [
        "Regresemos al ejemplo y recuperemos los √∫ltimos hidden states de un solo string. Lo primero que debemos hacer es codificar el string y convertir los tokens en tensores PyTorch. Esto se puede hacer proporcionando el argumento `return_tensors=\"pt\"` al tokenizador de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8ALFdmmgkMNN",
        "outputId": "7103e67c-e3b1-4388-a48d-76fe3a8646ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tensor shape: torch.Size([1, 6])\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "text = \"this is a test\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1ytd4xSkMNN"
      },
      "source": [
        "Como podemos ver, el tensor resultante tiene la forma `[batch_size, n_tokens]`. Ahora que tenemos las encodings como tensor, el paso final es colocarlas en el mismo dispositivo que el modelo y pasar los inputs de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vogg7OJNkMNN",
        "outputId": "e8db0d28-c7c8-45b2-c46e-3c318b612206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ...,\n",
            "-0.1188,  0.0662,  0.5470],\n",
            "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
            "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
            "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
            "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
            "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]]),\n",
            "hidden_states=None, attentions=None)\n"
          ]
        }
      ],
      "source": [
        "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWs4el6YkMNN"
      },
      "source": [
        "Aqu√≠ usamos el context manager `torch.no_grad()` para deshabilitar el c√°lculo autom√°tico de la gradiente. Esto es √∫til para el paso de inferencia ya que reduce la memoria de los c√°lculos. Seg√∫n la configuraci√≥n del modelo, el output puede contener varios objetos, como hidden states, losses o attentions, organizados en una clase similar a una `namedtuple` en Python. En nuestro ejemplo, el output del modelo es una instancia de `BaseModelOutput`, y podemos acceder a sus atributos por nombre. El modelo actual devuelve solo un atributo, que es el √∫ltimo hidden state, as√≠ que veamos su forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KR08ox8LkMNN",
        "outputId": "b29679ae-8864-4c5a-ff17-0fe430967223"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 768])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.last_hidden_state.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD5dUXKMkMNN"
      },
      "source": [
        "Viendo el tensor de hidden state, vemos que tiene la forma `[batch_size, n_tokens, hidden_dim]`. En otras palabras, devuelve un vector de 768 dimensiones para cada uno de los 6 tokens de input. Para las tareas de clasificaci√≥n, es una pr√°ctica com√∫n usar simplemente el hidden state asociado con el token `[CLS]` como input feature. Dado que este token aparece al comienzo de cada secuencia, podemos extraerlo simplemente index√°ndolo en `outputs.last_hidden_state`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3G13AmQ3kMNN",
        "outputId": "2a8625d0-8b31-4397-87ba-2563f3517d50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.last_hidden_state[:,0].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdxPaBtFkMNO"
      },
      "source": [
        "Ahora que sabemos c√≥mo sacar el √∫ltimo _hidden state_ para un solo string, hagamos lo mismo para todo el dataset creando una nueva columna `hidden_state` que almacene todos estos vectores. Como hicimos con el tokenizador, usamos el m√©todo `map()` de `DatasetDict` para extraer todos los hidden states de una vez. Lo primero que debemos hacer es meter los pasos anteriores en una funci√≥n de procesamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GP7toLBokMNO"
      },
      "outputs": [],
      "source": [
        "def extract_hidden_states(batch):\n",
        "    # Place model inputs on the GPU\n",
        "    inputs = {k:v.to(device) for k,v in batch.items() \n",
        "              if k in tokenizer.model_input_names}\n",
        "    # Extract last hidden states\n",
        "    with torch.no_grad():\n",
        "        last_hidden_state = model(**inputs).last_hidden_state\n",
        "    # Return vector for [CLS] token\n",
        "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzf9mw81kMNO"
      },
      "source": [
        "La √∫nica diferencia entre esta funci√≥n y nuestra l√≥gica anterior es el paso final en el que colocamos el hidden state final otra vez en el CPU como una matriz NumPy. El m√©todo `map()` requiere que la funci√≥n de procesamiento devuelva objetos Python o NumPy cuando usamos batched inputs.\n",
        "\n",
        "Dado que nuestro modelo espera tensores como inputs, lo siguiente que tenemos que hacer es convertir las columnas `input_ids` y `attention_mask` al formato `\"torch\"`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1LOFnE24kMNO"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfbf8ce393cf4ecb9287e8d3a0205a96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ed23f28195242528e70dedfce62adb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eded54a2a5df4d5599c0f9a3e80ce524",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f8009fe42e74e75944aa9776ed81000",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "emotions = load_dataset(\"dair-ai/emotion\")\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
        "emotions_encoded.set_format(\"torch\", \n",
        "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P36OKO7jkMNO"
      },
      "source": [
        "Luego podemos extraer el hidden state a traves de todos los splits en una sola pasada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "590842bb15bf448cb35e324e87fdadd9",
            "ee9bfe44feba4be69fa123fef39bce99",
            "e10764ed10b14c118c76a900bdf2752d"
          ]
        },
        "id": "UGCzfHTlkMNO",
        "outputId": "6376ee57-3b9b-4cbd-9bb9-4941d2d24201"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13421ea400ad4feaa14e9f359d119e02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "978914de68e44e6a9939d5f0a88229b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d0a27f408e14e8f97e60e5c6ebdfb91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bce9babaed345f980cab0c27083544a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'to'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\danie\\Desktop\\Repositorios\\IA\\02_classification_pt2.ipynb Cell 35\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m emotions_encoded \u001b[39m=\u001b[39m emotions\u001b[39m.\u001b[39mmap(tokenize, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m emotions_hidden \u001b[39m=\u001b[39m emotions_encoded\u001b[39m.\u001b[39;49mmap(extract_hidden_states, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\dataset_dict.py:486\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    484\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    485\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 486\u001b[0m     {\n\u001b[0;32m    487\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m    488\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[0;32m    489\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[0;32m    490\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[0;32m    491\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[0;32m    492\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    493\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[0;32m    494\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    495\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    496\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    497\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    498\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m    499\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[0;32m    500\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[0;32m    501\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[0;32m    502\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[0;32m    503\u001b[0m         )\n\u001b[0;32m    504\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    505\u001b[0m     }\n\u001b[0;32m    506\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\dataset_dict.py:487\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    484\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    485\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    486\u001b[0m     {\n\u001b[1;32m--> 487\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    488\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m    489\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m    490\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m    491\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m    492\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    493\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m    494\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m    495\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m    496\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[0;32m    497\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m    498\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m    499\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m    500\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m    501\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m    502\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m    503\u001b[0m         )\n\u001b[0;32m    504\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    505\u001b[0m     }\n\u001b[0;32m    506\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\arrow_dataset.py:2018\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2015\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(logging\u001b[39m.\u001b[39mget_verbosity() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m utils\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m   2017\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 2018\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[0;32m   2019\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m   2020\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m   2021\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m   2022\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m   2023\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m   2024\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2025\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m   2026\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m   2027\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   2028\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m   2029\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[0;32m   2030\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   2031\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   2032\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m   2033\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m   2034\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[0;32m   2035\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[0;32m   2036\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m   2037\u001b[0m     )\n\u001b[0;32m   2038\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2040\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\arrow_dataset.py:518\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    517\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    519\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    520\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    521\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\arrow_dataset.py:485\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    479\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    480\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    481\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    482\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    483\u001b[0m }\n\u001b[0;32m    484\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 485\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    486\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    487\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\fingerprint.py:411\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[0;32m    406\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[0;32m    407\u001b[0m             )\n\u001b[0;32m    409\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    413\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\arrow_dataset.py:2389\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   2385\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   2386\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(input_dataset\u001b[39m.\u001b[39mnum_rows)))\n\u001b[0;32m   2387\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[0;32m   2388\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2389\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[0;32m   2390\u001b[0m         batch,\n\u001b[0;32m   2391\u001b[0m         indices,\n\u001b[0;32m   2392\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(input_dataset\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[0;32m   2393\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[0;32m   2394\u001b[0m     )\n\u001b[0;32m   2395\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   2396\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   2397\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2398\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\arrow_dataset.py:2277\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[0;32m   2276\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[1;32m-> 2277\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39mfn_args, \u001b[39m*\u001b[39madditional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_kwargs)\n\u001b[0;32m   2278\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2279\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[0;32m   2280\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\arrow_dataset.py:1978\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[1;34m(item, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1974\u001b[0m decorated_item \u001b[39m=\u001b[39m (\n\u001b[0;32m   1975\u001b[0m     Example(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched \u001b[39melse\u001b[39;00m Batch(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[0;32m   1976\u001b[0m )\n\u001b[0;32m   1977\u001b[0m \u001b[39m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[1;32m-> 1978\u001b[0m result \u001b[39m=\u001b[39m f(decorated_item, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1979\u001b[0m \u001b[39m# Return a standard dict\u001b[39;00m\n\u001b[0;32m   1980\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, LazyDict) \u001b[39melse\u001b[39;00m result\n",
            "\u001b[1;32mc:\\Users\\danie\\Desktop\\Repositorios\\IA\\02_classification_pt2.ipynb Cell 35\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_hidden_states\u001b[39m(batch):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Place model inputs on the GPU\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     inputs \u001b[39m=\u001b[39m {k:v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems() \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m               \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m tokenizer\u001b[39m.\u001b[39mmodel_input_names}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Extract last hidden states\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
            "\u001b[1;32mc:\\Users\\danie\\Desktop\\Repositorios\\IA\\02_classification_pt2.ipynb Cell 35\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_hidden_states\u001b[39m(batch):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Place model inputs on the GPU\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     inputs \u001b[39m=\u001b[39m {k:v\u001b[39m.\u001b[39;49mto(device) \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems() \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m               \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m tokenizer\u001b[39m.\u001b[39mmodel_input_names}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Extract last hidden states\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/Repositorios/IA/02_classification_pt2.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
          ]
        }
      ],
      "source": [
        "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
        "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVzWVR2GkMNP"
      },
      "source": [
        "Tomen en cuenta que no configuramos `batch_size=None` en este caso, porque en su lugar se usa el valor default `batch_size=1000`. Aplicar la funci√≥n `extract_hidden_states()` ha agregado una nueva columna `hidden_state` a nuestro dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5idOTeQkMNP",
        "outputId": "cf57b1c4-e422-4fc2-9461-5064bc55a77b"
      },
      "outputs": [],
      "source": [
        "emotions_hidden[\"train\"].column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6zcV73LkMNP"
      },
      "source": [
        "Ahora que tenemos los hidden states asociados con cada tweet, el siguiente paso es entrenar un clasificador sobre ellos. Para hacer eso, vamos a necesitar un feature matrix:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTYV--2DkMNP"
      },
      "source": [
        "#### Creando un feature matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbu2Fg8ekMNP"
      },
      "source": [
        "El dataset preprocesado ahora tiene toda la informaci√≥n que necesitamos para entrenar un clasificador. Vamos a usar los hidden stattes como input features y las etiquetas como targets. Podemos crear f√°cilmente las matrices correspondientes en el formato Scikit-Learn asi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzguDZ-fkMNP",
        "outputId": "6b77c610-f61a-4c54-c169-33918a2080d0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
        "X_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
        "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
        "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
        "X_train.shape, X_valid.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4xzAIPtkMNP"
      },
      "source": [
        "Antes de entrenar un modelo en los hidden states, es una buena pr√°ctica hacer un sanity check para garantizar que proveen una representaci√≥n √∫til de las emociones que queremos clasificar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGJa8tTSkMNP"
      },
      "source": [
        "#### Visualizar el training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHyvVG4QkMNP"
      },
      "source": [
        "Dado que visualizar los estados ocultos en 768 dimensiones es complicado, usaremos el algoritmo UMAP[1] para proyectar los vectores en 2D. Dado que UMAP funciona mejor cuando los features se escalan para estar en el intervalo [0,1], primero aplicaremos un `MinMaxScaler` y luego usaremos la implementaci√≥n de UMAP de la libreria `umap-learn` para reducir los hidden states:\n",
        "\n",
        "L. McInnes, J. Healy, and J. Melville, [\"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\"](https://arxiv.org/abs/1802.03426), (2018)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE9iibF0kMNP",
        "outputId": "6438587a-a3b2-41f6-b1cb-e5b8668b1857"
      },
      "outputs": [],
      "source": [
        "from umap import UMAP\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Scale features to [0,1] range\n",
        "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
        "# Initialize and fit UMAP\n",
        "mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
        "# Create a DataFrame of 2D embeddings\n",
        "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
        "df_emb[\"label\"] = y_train\n",
        "df_emb.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Sx6AQ9kMNP"
      },
      "source": [
        "El resultado es una matriz con la misma cantidad de muestras de entrenamiento, pero con solo 2 features en lugar de las 768 con las que comenzamos. Investiguemos los datos comprimidos un poco m√°s y tracemos la densidad de puntos para cada categor√≠a por separado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0LXKQIZkMNP",
        "outputId": "1bc17274-9f01-4756-c766-87a1bea3c8b4"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(7,5))\n",
        "axes = axes.flatten()\n",
        "cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n",
        "labels = emotions[\"train\"].features[\"label\"].names\n",
        "\n",
        "for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
        "    df_emb_sub = df_emb.query(f\"label == {i}\")\n",
        "    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n",
        "                   gridsize=20, linewidths=(0,))\n",
        "    axes[i].set_title(label)\n",
        "    axes[i].set_xticks([]), axes[i].set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plIlmC8hkMNQ"
      },
      "source": [
        ">nota: Estas son solo proyecciones a un espacio de menor dimensi√≥n. El hecho de que algunas categor√≠as se superpongan no significa que no sean separables en el espacio original. Por otro lado, si son separables en el espacio proyectado, van a serlo en el espacio original.\n",
        "\n",
        "A partir de este plot podemos ver algunos patrones claros: los sentimientos negativos como \"sadness\", \"anger\" y \"fear\" ocupan regiones similares con distribuciones ligeramente diferentes. Por otro lado, la 'joy' y el 'love' est√°n bien separados de las emociones negativas y tambi√©n comparten un espacio similar. Finalmente, \"surprise\" est√° esparcida por todo el lugar. Aunque es posible que hayamos esperado cierta separaci√≥n, esto no est√° garantizado de ninguna manera ya que el modelo no fue entrenado para saber la diferencia entre estas emociones. Solo los aprendi√≥ impl√≠citamente al adivinar las palabras enmascaradas en los textos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMJomaZjkMNQ"
      },
      "source": [
        "#### Entrenando un clasificador simple\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_D8D2XCkMNQ"
      },
      "source": [
        "Hemos visto que los estados ocultos son algo diferentes entre las emociones, aunque para varias de ellas no existe un l√≠mite evidente. Usemos estos estados ocultos para entrenar un modelo de regresi√≥n log√≠stica con Scikit-Learn. El entrenamiento de un modelo tan simple es r√°pido y no necesita una GPU:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFtJuz8AkMNQ",
        "outputId": "58fdd2f6-9161-48fa-f6a9-158910a3941b"
      },
      "outputs": [],
      "source": [
        "# We increase `max_iter` to guarantee convergence \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_clf = LogisticRegression(max_iter=3000)\n",
        "lr_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5GIyq1ckMNQ",
        "outputId": "56984743-7a7b-43cf-82bf-313c5f27db9c"
      },
      "outputs": [],
      "source": [
        "lr_clf.score(X_valid, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5b6c21OkMNQ"
      },
      "source": [
        "En cuanto a la precisi√≥n, puede parecer que nuestro modelo es solo un poco mejor que el aleatorio, pero dado que estamos tratando con un conjunto de datos multiclase desequilibrado, en realidad es significativamente mejor. Podemos examinar si nuestro modelo es bueno compar√°ndolo con un baseline simple. En Scikit-Learn hay un `DummyClassifier` que se puede usar para construir un clasificador con heur√≠sticas simples, como elegir siempre la clase mayoritaria o dibujar siempre una clase aleatoria. En este caso, la heur√≠stica de mejor rendimiento es elegir siempre la clase m√°s frecuente, lo que tira una precisi√≥n de alrededor del 35%:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnnafMUGkMNQ",
        "outputId": "1f9cd157-78ef-4141-9ab5-8744ff447886"
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy_clf.fit(X_train, y_train)\n",
        "dummy_clf.score(X_valid, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLzdHFJ1kMNQ"
      },
      "source": [
        "Por lo tanto, nuestro clasificador simple con embeddings de DistilBERT es significativamente mejor que nuestra baseline. Podemos investigar m√°s a fondo el rendimiento del modelo mirando la matriz de confusi√≥n del clasificador, que nos dice la relaci√≥n entre las etiquetas verdaderas y las predecidas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYetsPWxkMNQ",
        "outputId": "e6a14ae3-80dd-49c4-f6d9-37834b8dc4b1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(y_preds, y_true, labels):\n",
        "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
        "    plt.title(\"Normalized confusion matrix\")\n",
        "    plt.show()\n",
        "    \n",
        "y_preds = lr_clf.predict(X_valid)\n",
        "plot_confusion_matrix(y_preds, y_valid, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C266aphkMNQ"
      },
      "source": [
        "Podemos ver que \"anger\" y \"fear\" se confunden con mayor frecuencia con \"sadness\", lo que concuerda con la observaci√≥n que hicimos al visualizar los embeddings. Adem√°s, 'love' y 'surprise' se confunden con frecuencia con 'joy'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp9W4ATKkMNQ"
      },
      "source": [
        "En la siguiente parte, vamos a ver el enfoque de fine-tunning, que nos da un rendimiento de clasificaci√≥n superior. Sin embargo, es importante tener en cuenta que hacer esto requiere m√°s recursos, como GPUs, que podr√≠an no estar disponibles en su empresa. En casos como estos, un enfoque basado en features puede ser un buen compromiso entre ML tradicional y DL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVH9aSRQkMNQ"
      },
      "source": [
        "### Fine-Tuning Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-6QWcTTkMNQ"
      },
      "source": [
        "Exploremos ahora lo que se necesita para ajustar un transformer end-to-end. Con el enfoque de fine-tuning, no usamos los estados ocultos como features fijos, sino que los entrenamos. Esto requiere que la cabeza de clasificacion sea diferenciable, por esto, este m√©todo suele utilizar una red neuronal para la clasificaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcRe84iDkMNQ"
      },
      "source": [
        "Entrenar los estados ocultos que sirven como inputs para el modelo de clasificaci√≥n nos ayuda a evitar el problema de trabajar con datos que pueden no ser adecuados para la tarea de clasificaci√≥n. En cambio, los estados ocultos iniciales se adaptan durante el entrenamiento para disminuir la p√©rdida del modelo y as√≠ aumentar su rendimiento.\n",
        "\n",
        "Usaremos la API `Trainer` de Transformers para simplificar el training loop. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTAzVjPWkMNQ"
      },
      "source": [
        "#### Cargando un modelo preentrenado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-W1ZdGzkMNQ"
      },
      "source": [
        "Lo primero que necesitamos es un modelo DistilBERT preentrenado. La √∫nica peque√±a modificaci√≥n es que usamos el modelo `AutoModelForSequenceClassification` en lugar de `AutoModel`. La diferencia es que el modelo `AutoModelForSequenceClassification` tiene una cabeza de clasificacion sobre los outputs del modelo preentrenado, que se pueden entrenar f√°cilmente con el modelo base. Solo necesitamos especificar cu√°ntas etiquetas tiene que predecir el modelo (seis en nuestro caso), ya que esto dicta la cantidad de outputs que tiene la cabeza de clasificacion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sWSSnx0kMNQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 6\n",
        "model = (AutoModelForSequenceClassification\n",
        "         .from_pretrained(model_ckpt, num_labels=num_labels)\n",
        "         .to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCnwFcFWkMNQ"
      },
      "source": [
        "Vamos a ver una advertencia de que algunas partes del modelo se inicializan aleatoriamente. Esto es normal ya que la cabeza de clasificacion no ha sido entrenada. El siguiente paso es definir las m√©tricas que usaremos para evaluar el rendimiento de nuestro modelo durante el ajuste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLP2crbpkMNR"
      },
      "source": [
        "#### Definiendo las metricas de rendimiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsIIECSRkMNR"
      },
      "source": [
        "Para monitorear las m√©tricas durante el entrenamiento, necesitamos definir una funci√≥n `compute_metrics()` para el `Trainer`. Esta funci√≥n recibe un objeto `EvalPrediction` (que es un named tuple con atributos `predictions` y `label_ids`) y necesita devolver un diccionario que asigna el nombre de cada m√©trica a su valor. Para nuestra aplicaci√≥n, calcularemos la puntuaci√≥n de $F_1$ y la precisi√≥n del modelo:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG0f4MjukMNR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWkRVx_kkMNR"
      },
      "source": [
        "Con el dataset y las m√©tricas listos, solo tenemos dos cosas finales antes de definir la clase `Trainer`:\n",
        "\n",
        "1. Iniciar sesi√≥n en Hugging Face Hub. Esto nos permitir√° cargar nuestro modelo a nuestra cuenta en Hub y compartirlo.\n",
        "2. Definir todos los hiperpar√°metros para el training run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddmVOYohkMNR"
      },
      "source": [
        "#### Entrenando el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mQHLJM_kMNR"
      },
      "source": [
        "Para hacer login desde Colab, pueden usar esta funcion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37YGQqkFkMNR"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kK8WYhPkMNR"
      },
      "source": [
        "Esto mostrar√° un widget en el que puede ingresar su usuario y contrase√±a, o un token de acceso con privilegios de escritura. Pueden encontrar detalles sobre c√≥mo crear tokens de acceso en la [documentaci√≥n de Hub](https://huggingface.co/docs/hub/security#user-access-tokens). Si est√°n trabajando en la terminal, pueden iniciar sesi√≥n ejecutando el siguiente comando:\n",
        "\n",
        "```bash\n",
        "$ huggingface-cli login\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzTM8T46kMNR"
      },
      "source": [
        "Para definir los par√°metros de entrenamiento, usamos la clase `TrainingArguments`. Esta clase almacena mucha informaci√≥n y nos da control detallado sobre el entrenamiento y la evaluaci√≥n. El argumento m√°s importante para especificar es `output_dir`, que es donde se almacenan todos los artefactos del entrenamiento. Aqu√≠ hay un ejemplo de `TrainingArguments`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIqz4kDVkMNR"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "batch_size = 64\n",
        "logging_steps = len(emotions_encoded[\"train\"]) // batch_size\n",
        "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
        "training_args = TrainingArguments(output_dir=model_name,\n",
        "                                  num_train_epochs=2,\n",
        "                                  learning_rate=2e-5,\n",
        "                                  per_device_train_batch_size=batch_size,\n",
        "                                  per_device_eval_batch_size=batch_size,\n",
        "                                  weight_decay=0.01,\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  disable_tqdm=False,\n",
        "                                  logging_steps=logging_steps,\n",
        "                                  push_to_hub=True, \n",
        "                                  log_level=\"error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p7VCABikMNR"
      },
      "source": [
        "Aqu√≠ tambi√©n establecemos el tama√±o del batch, learning rate y el n√∫mero de √©pocas, y especificamos cargar el mejor modelo al final del training run. Con esto, podemos instanciar y afinar nuestro modelo con el `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QqxvODFkMNR",
        "outputId": "44bdf908-bdd9-4ea4-aea6-de42f03e61d0"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, \n",
        "                  compute_metrics=compute_metrics,\n",
        "                  train_dataset=emotions_encoded[\"train\"],\n",
        "                  eval_dataset=emotions_encoded[\"validation\"],\n",
        "                  tokenizer=tokenizer)\n",
        "trainer.train();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C21mgLVCkMNR"
      },
      "source": [
        "Si observamos los logs, podemos ver que nuestro modelo tiene una puntuaci√≥n de $F_1$ en el dataset de validaci√≥n de alrededor de 92%. Esta es una mejora significativa con respecto al enfoque basado en features.\n",
        "\n",
        "Podemos echar un vistazo m√°s detallado a las m√©tricas de entrenamiento calculando la matriz de confusi√≥n. Para visualizar la matriz de confusi√≥n, primero debemos obtener las predicciones en el conjunto de validaci√≥n. El m√©todo `predict()` de la clase `Trainer` devuelve varios objetos √∫tiles que podemos usar para la evaluaci√≥n:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B-syT6gkMNR",
        "outputId": "ffb749b5-8405-4e3b-991e-0318a897ccd3"
      },
      "outputs": [],
      "source": [
        "# hide_output\n",
        "preds_output = trainer.predict(emotions_encoded[\"validation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0yL57CskMNR"
      },
      "source": [
        "El resultado del m√©todo `predict()` es un objeto `PredictionOutput` que contiene matrices de `predictions` y `label_ids`, junto con las m√©tricas que le pasamos al trainer. Por ejemplo, se puede acceder a las m√©tricas en el dataset de validaci√≥n asi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvqrAeXmkMNR",
        "outputId": "458f6ecf-3100-4b99-e9ee-aa084e5dd990"
      },
      "outputs": [],
      "source": [
        "preds_output.metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLcO8xnykMNR"
      },
      "source": [
        "Tambi√©n tiene las predicciones sin procesar para cada clase. Podemos decodificar las predicciones usando `np.argmax()`. Esto produce las etiquetas predecidas y tiene el mismo formato que las etiquetas devueltas por los modelos de Scikit-Learn en el enfoque basado en features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF62rZXjkMNR"
      },
      "outputs": [],
      "source": [
        "y_preds = np.argmax(preds_output.predictions, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NKPdM3GkMNR"
      },
      "source": [
        "Con las predicciones podemos generar la matriz de confusion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJY6yy35kMNR",
        "outputId": "d16ccb13-8818-409b-d4db-23c5966ee72e"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_preds, y_valid, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u763pR3bkMNS"
      },
      "source": [
        "Esto est√° mucho m√°s cerca de la matriz de confusi√≥n diagonal ideal. La categor√≠a 'love' todav√≠a se confunde a menudo con 'joy', lo que parece natural. \"surprise\" tambi√©n se confunde con frecuencia con \"joy\", o se confunde con \"fear\". En general, el rendimiento del modelo parece bastante bueno, pero antes de darlo por terminado, profundicemos un poco m√°s en los tipos de errores que es probable que cometa nuestro modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9gXCKW8kMNS"
      },
      "source": [
        "#### Analisis de errores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpddpFm7kMNS"
      },
      "source": [
        "Antes de continuar, debemos investigar un poco m√°s las predicciones de nuestro modelo. Una t√©cnica simple pero poderosa es ordenar las muestras de validaci√≥n por el loss del modelo. Cuando pasamos la etiqueta durante el pase hacia adelante, el loss se calcula y se devuelve autom√°ticamente. Aqu√≠ hay una funci√≥n que devuelve el loss junto con la etiqueta predecida:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahz1itQbkMNS"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "def forward_pass_with_label(batch):\n",
        "    # Place all input tensors on the same device as the model\n",
        "    inputs = {k:v.to(device) for k,v in batch.items() \n",
        "              if k in tokenizer.model_input_names}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "        pred_label = torch.argmax(output.logits, axis=-1)\n",
        "        loss = cross_entropy(output.logits, batch[\"label\"].to(device), \n",
        "                             reduction=\"none\")\n",
        "\n",
        "    # Place outputs on CPU for compatibility with other dataset columns   \n",
        "    return {\"loss\": loss.cpu().numpy(), \n",
        "            \"predicted_label\": pred_label.cpu().numpy()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnrwkZVlkMNS"
      },
      "source": [
        "Usando `map()` otra vez, podemos aplicar esta funcion para obtener el loss de todas las muestras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6004443ac1344ee18d40c8a90c1178f4"
          ]
        },
        "id": "ySOwldfkkMNS",
        "outputId": "28f6389a-a309-4830-ea7c-6f5a8a32e45b"
      },
      "outputs": [],
      "source": [
        "# Convert our dataset back to PyTorch tensors\n",
        "emotions_encoded.set_format(\"torch\", \n",
        "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "# Compute loss values\n",
        "emotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n",
        "    forward_pass_with_label, batched=True, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j--NTookMNS"
      },
      "source": [
        "Finalmente, creamos un `DataFrame` con los textos, losses y etiquetas predecidas/reales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCgs5FqXkMNS"
      },
      "outputs": [],
      "source": [
        "emotions_encoded.set_format(\"pandas\")\n",
        "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
        "df_test = emotions_encoded[\"validation\"][:][cols]\n",
        "df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
        "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n",
        "                              .apply(label_int2str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdWACxjXkMNT"
      },
      "source": [
        "Ahora podemos ordenar f√°cilmente `emotions_encoded` por las p√©rdidas en orden ascendente o descendente. El objetivo de este ejercicio es detectar uno de los siguientes:\n",
        "\n",
        "- _Etiquetas incorrectas_: todos los procesos que agregan etiquetas a los datos pueden tener fallas. Los anotadores pueden cometer errores o no estar de acuerdo, mientras que las etiquetas que se infieren de otros features pueden estar equivocadas. Si fuera f√°cil anotar datos autom√°ticamente, entonces no necesitar√≠amos un modelo para hacerlo. Por lo tanto, es normal que existan algunos ejemplos mal etiquetados. Con este enfoque, podemos encontrarlos y corregirlos r√°pidamente.\n",
        "\n",
        "- _Caracter√≠sticas del dataset_: Los datasets en el mundo real siempre son un poco desordenados. Cuando se trabaja con texto, los caracteres especiales o strings en los inputs pueden tener un gran impacto en las predicciones del modelo. Inspeccionar las predicciones m√°s d√©biles del modelo puede ayudar a identificar dichas caracter√≠sticas, y limpiar los datos o inyectar ejemplos similares puede hacer que el modelo sea m√°s s√≥lido.\n",
        "\n",
        "Primero veamos las muestras de datos con las mayores loss:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNE7hT2lkMNT",
        "outputId": "99a6d2b0-c611-4958-81a5-a53bce0b2dd7"
      },
      "outputs": [],
      "source": [
        "df_test.sort_values(\"loss\", ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XnKLWsYkMNT"
      },
      "source": [
        "Podemos ver claramente que el modelo predijo incorrectamente algunas de las etiquetas. Por otro lado, parece que hay bastantes ejemplos sin una clase clara, que podr√≠an estar mal etiquetados o requerir una nueva clase por completo. En particular, \"joy\" parece estar mal etiquetado varias veces. Con esta informaci√≥n podemos refinar el conjunto de datos, lo que a menudo puede conducir a una ganancia de rendimiento tan grande (o m√°s) como tener m√°s datos o modelos m√°s grandes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibWyjwaqkMNT"
      },
      "source": [
        "Al observar las muestras con el loss m√°s bajo, observamos que el modelo parece tener m√°s confianza al predecir la clase de \"sadness\". Los modelos de DL son muy buenos para encontrar y explotar shortcuts para llegar a una predicci√≥n. Por esta raz√≥n, tambi√©n vale la pena invertir tiempo en ver los ejemplos en los que el modelo tiene m√°s confianza, para que podamos estar seguros de que el modelo no explota indebidamente ciertas caracter√≠sticas del texto. Entonces, veamos tambi√©n las predicciones con el loss m√°s peque√±o:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLd2lVCAkMNT",
        "outputId": "08a1ede2-6af6-48fd-d49b-78ed7ade12f3"
      },
      "outputs": [],
      "source": [
        "df_test.sort_values(\"loss\", ascending=True).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghn9tdNbkMNT"
      },
      "source": [
        "Ahora sabemos que \"joy\" a veces est√° mal etiquetada y que el modelo conf√≠a m√°s en predecir la etiqueta \"sadness\". Con esta informaci√≥n, podemos realizar mejoras espec√≠ficas en nuestro dataset y tambi√©n vigilar la clase en la que el modelo parece tener mucha confianza.\n",
        "\n",
        "El √∫ltimo paso antes de servir el modelo entrenado es guardarlo para su uso posterior. Transformers nos permite hacer esto en unn par de pasos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYrWP2EHkMNT"
      },
      "source": [
        "#### Guardando y compartiendo el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4FuvU6XkMNT"
      },
      "source": [
        "La comunidad de NLP se beneficia bastante al compartir modelos previamente entrenados y ajustados, y todos pueden compartir sus modelos con otros usando Hugging Face Hub. Cualquier modelo generado por la comunidad se puede descargar del Hub tal como descargamos el modelo DistilBERT. Con la API `Trainer`, guardar y compartir un modelo es facil:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyVjhQ8_kMNT"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(commit_message=\"Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAwpjV02kMNT"
      },
      "source": [
        "Tambi√©n podemos usar el modelo ajustado para hacer predicciones sobre nuevos tweets. Como hemos enviado nuestro modelo al Hub, ahora podemos usarlo con la funci√≥n `pipeline()`, tal como lo hicimos en el notebook de intro. Primero, carguemos el pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vnpfs3c6kMNT"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Change `my_user` to your Hub username\n",
        "model_id = \"MY_USER/distilbert-base-uncased-finetuned-emotion\"\n",
        "classifier = pipeline(\"text-classification\", model=model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoQEhoYkkMNT"
      },
      "source": [
        "Ahora probemos el pipeline con un tweet de muestra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCMXTxvRkMNT"
      },
      "outputs": [],
      "source": [
        "custom_tweet = \"I saw a movie today and it was really good.\"\n",
        "preds = classifier(custom_tweet, return_all_scores=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0_3frTrkMNT"
      },
      "source": [
        "Finalmente, podemos plotear la probabilidad de cada clase en un gr√°fico de barras. Claramente, el modelo estima que la clase m√°s probable es \"joy\", lo que parece razonable dado el tuit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KNPHLWrkMNT",
        "outputId": "b052a907-808b-4f55-80ca-fe97768750ef"
      },
      "outputs": [],
      "source": [
        "preds_df = pd.DataFrame(preds[0])\n",
        "plt.bar(labels, 100 * preds_df[\"score\"], color='C0')\n",
        "plt.title(f'\"{custom_tweet}\"')\n",
        "plt.ylabel(\"Class probability (%)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAqDm8G_7ZqY"
      },
      "source": [
        "#### Fine-tunning con Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wAOoATP7tKW"
      },
      "source": [
        "Si usan TensorFlow, tambi√©n es posible ajustar sus modelos con la API de Keras. La principal diferencia con la API de PyTorch es que no hay una clase de `Trainer`, ya que los modelos de Keras ya proporcionan un m√©todo `fit()` integrado. Para ver c√≥mo funciona esto, primero carguemos DistilBERT como un modelo de TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyzoXzmA74od"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "tf_model = (TFAutoModelForSequenceClassification\n",
        "            .from_pretrained(model_ckpt, num_labels=num_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95_nW4bv792l"
      },
      "source": [
        "Luego, vamos a convertir nuestros datasets al formato `tf.data.Dataset`. Como ya aplicamos padding a los inputs tokenizados, podemos hacerlo f√°cilmente aplicando el m√©todo `to_tf_dataset()` a las emociones codificadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JmkEcFC79bE"
      },
      "outputs": [],
      "source": [
        "# The column names to convert to TensorFlow tensors\n",
        "tokenizer_columns = tokenizer.model_input_names\n",
        "\n",
        "tf_train_dataset = emotions_encoded[\"train\"].to_tf_dataset(\n",
        "    columns=tokenizer_columns, label_cols=[\"label\"], shuffle=True,\n",
        "    batch_size=batch_size)\n",
        "tf_eval_dataset = emotions_encoded[\"validation\"].to_tf_dataset(\n",
        "    columns=tokenizer_columns, label_cols=[\"label\"], shuffle=False,\n",
        "    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDLRej58YjB"
      },
      "source": [
        "Aqu√≠ tambi√©n hicimos shuffle del dataset de entrenamiento y definimos el tama√±o del batch para este dataset y el de validacion. Lo √∫ltimo que tenemos que hacer es compilar y entrenar el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZcuaIRh8xxw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy())\n",
        "\n",
        "tf_model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncIYuvY983a7"
      },
      "source": [
        "`model.compile()` es el unico paso adicional que expone el API de Keras a diferencia del API de Scikit-learn. En el metodo `compile()` especificamos el optimizador, la funcion de costo y las metricas a tomar en cuenta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUBzGl6LkMNT"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kE1JCQzkMNT"
      },
      "source": [
        "Ahora saben c√≥mo entrenar un transformer para clasificar las emociones en los tweets. Hemos visto dos enfoques complementarios basados ‚Äã‚Äãen features y fine-tuning, e investigamos sus fortalezas y debilidades.\n",
        "\n",
        "Sin embargo, este es solo el primer paso en la creaci√≥n de una aplicaci√≥n del mundo real con transformers. Aqu√≠ hay una lista de desaf√≠os que es probable que experimenten en NLP:\n",
        "\n",
        "Mi jefe quiere mi modelo en producci√≥n ayer:\n",
        "En la mayor√≠a de las aplicaciones, nuestro modelo no solo se sienta en alg√∫n notebook, sino que queremos asegurarnos de que est√© sirviendo predicciones. Cuando enviamos un modelo al Hub, se crea autom√°ticamente un endpoihnt de inferencia al que se puede llamar con HTTP requests. Vean la [documentaci√≥n](https://api-inference.huggingface.co/docs/python/html/index.html) de la API de inferencia para saber mas.\n",
        "\n",
        "¬°Mis usuarios quieren predicciones m√°s r√°pidas!:\n",
        "Ya hemos visto un enfoque para este problema: usar DistilBERT. Mas adelante vamos a ver _knowledge distillation_ (el proceso mediante el cual se cre√≥ DistilBERT), junto con otros trucos para acelerar sus transformers.\n",
        "\n",
        "\n",
        "¬øTu modelo tambi√©n puede hacer X?:\n",
        "Como hemos mencionado en la clase, los transformers son extremadamente vers√°tiles. En el resto de clases, vamos a explorar una variedad de tareas, como question answering y named entity recognition, todas usando la misma arquitectura b√°sica.\n",
        "\n",
        "¬°Ninguno de mis textos est√° en ingl√©s!:\n",
        "Los transformers tambi√©n vienen en una variedad multiling√ºe, y los vamos a usar para NER para abordar varios idiomas a la vez.\n",
        "\n",
        "¬°No tengo ninguna etiqueta!:\n",
        "Si hay muy pocos datos etiquetados disponibles, es posible que el fine-tuning no sea una opci√≥n. Mas adelante, vamos a ver algunas t√©cnicas para lidiar con esta situaci√≥n.\n",
        "\n",
        "Ahora que hemos visto lo que implica entrenar y compartir un transformer, en las proximas clases vamos a ver la implementaci√≥n de nuestro propio modelo transformer desde cero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IgcJjVykMNU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "02_classification_pt2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
